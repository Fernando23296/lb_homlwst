{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El paisaje de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la actualidad las personas tienen una idea del ML muy enlazada a una imagen de un robot. Pero ML no es solo un futuro fantasioso, en verdad ya esta en nuestro presente. De hecho ya ha estado por muchas decadas en aplicaciones especializadas, tales como el OCR (Optical Character Recognition). Pero la verdadera primera vez que ML se volvio famoso fue con el spam filter.<br>\n",
    "¿Cuando el ML empieza y cuando termina? Que significa realmente que una maquina \"aprenda\". Si descargo algo de wikipedia ¿mi maquina ya aprendio algo?<br>\n",
    "#### ¿Que es Machine Learning?<br>\n",
    "ML es el arte de programar computadoras tal que estas aprendan a partir de los datos.<br>\n",
    "Por ejemplo el filtro de spam es un programa de ML porque puede aprender a identificar spam a partir de ejemplos de correos de spam (por ejemplo aquellos que fueron identificados por los usuarios) y ejemplos de emails regulares (que no son spam). Los ejemplos que el sistema usa para aprender son llamados <b>training set</b>. Cada ejemplo de entrenamiento es llamado <b>training instance</b>(o ejemplo). En este caso, la tarea T (haciendo referencia a la definicion de Tom Mitchell) es el marcar si es o no spam a los nuevos emails, la experiencia E es el <b>training data</b> y el desempeño P necesita ser definido.; por ejemplo se puede usar la proporción de correos electrónicos correctamente clasificados.<br>\n",
    "Esta medida de desempeño es llamado accuracy (exactitud) y es usado en tareas de clasificación.\n",
    "<br>\n",
    "#### ¿Por qué usar ML?\n",
    "Como hariamos para escribir un filtro de spam usando la tecnica de programacion tradicional:<br>\n",
    "1.- Primero se ve como luce un mensaje de spam. Acá nos damos cuenta sobre palabras y frases (\"credit card\",\"free\",\"amazing\") que aparecen en el subject. Tal vez, nos encontremos con otros patrones en el nombre de remitente, el email y más. <br>\n",
    "2.- Luego se debera escribir un algoritmo de deteccion para cada uno de los patrones que hemos identificado, y el programa marcaria los emauls como spam si un numero de estos patrones es detectado.<br>\n",
    "3.- Luego se procederia a testear el programa y repetir los pasos 1 y 2 hasta que este lo suficientemente bien.<br>\n",
    "<br>\n",
    "Ya que el problema no es trivial, el programa eventualmente hará crecer la lista de reglas, lo cual es dificil de mantener.\n",
    "En contraste, un filtro de spam basado en tecnicas de ML automaticamente aprende que palabras y frases son buenos predictores de spam mediante la deteccion de patrones frecuentes inusuales de palabras en los ejemplos de spam comparados con los que no son spam. El programa es mucho mas corto y sencillo de mantener, y muchas veces mas preciso.<br>\n",
    "Otra situacion seria la de que el Spammer se da cuenta que al usar \"4U\" su mensaje esta siendo enviado a la seccion de spam, por lo cual dejaria de usar \"4U\" y lo cambiaria a \"For you\". Si estamos programando nuestro detector de spam en la manera convencional entonces el programador debera cambiar el \"4U\" por \"For you\". Si el spammer sigue cambiando el programador tendra que hacer esto por siempre. <br>\n",
    "En contraste al filtro de spam basado en ML, este automaticamente se da cuenta que \"For u\" ha empezado a ser muy frecuentemente marcado como spam por los usuarios, entonces lo clasificaria dentro de la categoria de spam, sin necesidad de la ayuda del programador.<br>\n",
    "Otra area donde el Machine Learning brilla es para problemas que o bien son muy complejos con enfoques tradicionales o no tienen un algoritmo conocido. <br>\n",
    "El ML tambien ayuda a los humanos a aprender. Continuando con el ejercicio anterior, el algoritmo de ML puede ser inspeccionado para ver como ha aprendido, en este caso, ver que palabras y combinaciones de palabras cree que son propias de un mensaje de spam. A veces esto revela nuevas tendencias, y a la vez una nueva manera de entender un problema.<br>\n",
    "Aplicar tecnicas de ML en grandes cantidades de datos puede ayudar a descubrir patrones que no son visibles a la primera. Esto es llamado <b>data mining</b>.\n",
    "Para resumir:\n",
    "<ul>\n",
    "    <li>Problemas donde ela solucion requiera mucha intervencion manual en modificar listas de reglas: un algortimo de ML podria simplificar el codifo y tener mejor desempeño.</li>\n",
    "    <li>Problemas complejos donde parece no haber una buena solucion, las mejores tecnicas de ML pueden encontrar una solucion</li>\n",
    "    <li>Un sistema de ML puede adaptarse a nuevos datos</li>\n",
    "</ul>\n",
    "\n",
    "#### Tipos de Sistemas de ML\n",
    "Hay tantos tipos diferentes de sistemas de ML, por lo cual lo clasiificaremos en categorias basadas en:\n",
    "<ul>\n",
    "    <li>Ya sea o no que se tendra entrenamiento con supervision humana (supervisado, nosupervisado, semisupervisado y aprendizaje reforzado)</li>\n",
    "    <li>Ya sea o no que aprendera en el camino(online versus batch learning)</li>\n",
    "    <li>Ya sea que su trabajo consiste en comparar nuevos datos con antiguos datos, o detectan patrones en el <b>training data</b> y construyen un modelo predictivo (instance-based versus model-based learning)</li>\n",
    "</ul>\n",
    "Estos criterios no son exclusivos, pueden ser combinados como uno quiere. Por ejemplo una manera de hacer el filtro de spam seria que este aprenda en el \"camino\" usando un modelo de red neuronal de Deep Learning, que esta entrenado con ejemplos de spam y nospam; todo esto lo hace un sistema online con aprendizaje supervisado. <br>\n",
    "\n",
    "#### Aprendizaje Supervisado/No Supervisado\n",
    "<b>Aprendizaje Supervisado:</b> En este aprendizaje el training data que le damos al algoritmo incluye las \"soluciones deseadas\", estas son llamadas labels. Una tarea muy tipica en el aprendizaje supervisado es el de la clasificacion. El filtro de spam es un buen ejemplo de esto. Otra tarea muy tipica es el de predecir un <b>target</b> el cual es un valor numerico, como el precio de un auto, dados un conjunto de caracteristicas (kilometraje, año del modelo, marca,etc) lo cuales son llamados predictores. Este tipo de tarea es llamado regresion.<br>\n",
    "En ML un atributo es un tipo de dato (por ejemplo: Kilometraje), mientras que un feature (caracteristica) tiene muchos significados dependiendo el contextol, pero por lo general significa un atributo mas su valor: \"Kilometraje\" = 15000. <br>\n",
    "Notar que algunos algoritmos de regresion pueden ser usados para clasificacion tambien, y viceversa. Por ejemplo, Regresion Logistica es comunmente usado para la clasificacion, y su output puede ser la probabilidad de pertenecer a X clase, por ejemplo 20% de ser spam.<br>\n",
    "Algunos de los algoritmos mas importantes de Aprendizaje Supervisado son:\n",
    "<ul>\n",
    "    <li>K-Nearest Neighbors</li>\n",
    "    <li>Linear Regression</li>\n",
    "    <li>Logistic Regression</li>\n",
    "    <li>Support Vector Machines (SVM)</li>\n",
    "    <li>Decision Trees y Random Forests</li>\n",
    "    <li>Redes Neuronales (algunos son de aprendizaje supervisado, como los autoencoders y las maquinas Boltzmann)</li>\n",
    "</ul>\n",
    "<b>Aprendizaje No Supervisado</b>: En el aprendizaje no supervisado, tendras que adivinar, el datos de entrenamiento (training data) no esta etiquetado. El sistema trata de entender sin ningun profesor.<br>\n",
    "Algunos de los mas importantes algoritmos de aprendizaje no supervisado son:\n",
    "<ul> \n",
    "    <li>Clustering</li>\n",
    "        <ul> \n",
    "            <li>k-Means</li>\n",
    "            <li>Hierarchical Cluster Analysis (HCA)</li>\n",
    "            <li>Expectation Maximization</li>\n",
    "        </ul>\n",
    "    \n",
    "  <li>Visualization and dimensionality reduction</li>\n",
    " <ul> \n",
    "            <li>Principal Component Analysis (PCA)</li>\n",
    "            <li>Kernel PCA</li>\n",
    "            <li>Locally-Linear Embedding(LLE)</li>\n",
    "            <li>t-distributed Stochastic Neighbor Embedding (t-SNE)</li>\n",
    " </ul>\n",
    " \n",
    " \n",
    "   <li>Asociacion de reglas de aprendizaje</li>\n",
    " <ul> \n",
    "            <li>Apriori</li>\n",
    "            <li>Eclat</li>\n",
    " </ul>\n",
    " \n",
    "</ul>\n",
    "\n",
    "Digamos que se tiene un gran conjunto de datos sobre visitantes de nuestro blog, si vamos a usar un algoritmo de aprendizaje no supervisado entonces no es necesario \"decirle\" nada, puesto que este algoritmo entendera por manera propia el como manejar estos datos, en este caso encontrara patrones. \n",
    "Los algoritmos de visualizacion son buenos ejemplos de este tipo de aprendizaje, pues ayudan a entender de mejor manera los agrupamientos que el algoritmo va encontrando.<br>\n",
    "Una tarea relacionada es la de la reduccion de dimensionalidad, en el cual el objetivo es el de simplificar los datos, sin perder mucha informacion. Una manera de hacer esto es unir varias caracteristicas correlacionadas en una. Por ejemplo el kilometraje de un auto puede ser muy correlacionada con su \"edad\" asi que el algoritmo de reduccion puede unir todos estos en una caracteristica que representa el desgaste del auto. Esto es llamado <b>feature extraction.</b><br>\n",
    "Es casi siempre una buena idea el tratar de reducir la dimension del trainig data usando un algoritmo de reduccion de dimensionalidad, antes de pasarselo a un algoritmo de ML. Se ejecutará de una manera mas rapida, los datos ocuparan menos espacio y en muchos casos tendran mejor rendimiento.<br>\n",
    "Otra importante tarea de los algoritmos de aprendizaje no supervisado es el de la deteccion de anomalias, por ejemplo el de detectar una transaccion inusual de una tarjeta de credito para asi prevenir fraude. El sistema esta entrenado con instancias normales, y cuando ve una nueva instancia, este sistema puede decir si es normal o es una anomalia.<br>\n",
    "Otro algoritmo importante es el de Association Rule Learning, el cual tiene el objetivo de descubrir relaciones entre los atributos. Por ejemplo en el dataset de un supermercado, puede ver la relacion entre comprar carne de parrilla con la de las papas fritas.<br>\n",
    "#### Aprendizaje SemiSupervisado\n",
    "Algunos algoritmos pueden lidiar con training data etiquetado y la otra parte sin etiquetar. Esto es conocido como <b>semisupervised learning.</b><br>\n",
    "Un ejemplo claro de esto es los servicios donde se guardan las fotos, por ejemplo Google Photos, donde en un conjunto de fotos, una persona A aparece en las fotos 2,5,9 mientras otra persona B aparece en las fotos 4,6,8. El sistema necesita que alguien le diga quienes son estas personas, basta con \"etiquetar\" el nombre en una foto para que este se replique en las otras donde aparece. Lo cual es muy util para buscar fotos.<br>\n",
    "Muchos algoritmos de aprendizaje semisupervisado son combinaciones de algoritmos supervisados y no supervisados. Por ejemplo el <b>Deep Belief Networks (DBNs)</b> esta basado en componentes no supervisados llamados <b>restricted Boltzmann machines (RBMs)</b> posicionados uno encima de otro. En los RBMs el entrenamiento se da de manera secuencial, usando un metodo no supervisado, y despues todo el sistema en conjunto esta \"tuneado\" usando tecnicas de aprendizaje supervisado.<br>\n",
    "\n",
    "#### Aprendizaje Reforzado\n",
    "En este tipo de aprendizaje, se tiene un sistema de aprendizaje llamado \"agente\" el cual observa el entorno, selecciona y realiza acciones, al hacer esto obtiene recompensas o penalties (en el aspecto negativo).  Tiene que aprender por cuenta propia, cual es la mejor estrategia a realizar, lo cual es llamado \"policy\", para asi tener el mayor numero de recompensas por periodo. Una \"policy\" define que accion el agente debe elegir dada una situacion.<br>\n",
    "Muchos robots usan este tipo de aprendizaje para aprender a caminar, un ejemplo claro tambien es el DeepMind's Alpha Go, que aprendio a jugar analizando milllones de jugadas.<br>\n",
    "\n",
    "#### Batch and Online Learning\n",
    "Un criterio usado para clasificar sistemas de ML es si el sistema puede o no incrementar su aprendizaje a partir de un flujo entrante de datos.\n",
    "\n",
    "##### Batch Learning\n",
    "En Batch Learning, el sistema es incapaz de incrementar el aprendizaje: lo cual significa que entrenará usando todos los datos disponibles. Esto tomará un largo tiempo y recuersos de computacion, por lo cual se lo realiza de manera offline. Primero el sistema es entrenado y despues en lanzado a produccion donde se ejecuta sin volver a aprender, solo aplica lo que aprendio, esto es llamado offline learning.<br>\n",
    "Si queremos que este aprenda con nuevos datos (como el de un nuevo tipo de spam), necesitaremos entrenar una nueva version del sistema desde cero con un dataset completo (no solo los nuevos datos, los antiguos tambien), despues le pondriamos pausa al antiguo sistema para reemplazarlo con el nuevo. <br>\n",
    "Afortunadamente el proceso de entrenamiento, evaluacion y lanzamiento del sistema de ML puede ser facilmente automatizado, asi que incluso un sistema de Batch Learning puede ser adaptado al cambio.<br>\n",
    "Esta solucion es simple y casi siempre funciona bien, pero toma mucho tiempo entrenar todo el dataset completo cada vez, lo cual tambien consume mucho equipo de hardware (CPU, espacio en la memoria, espacio en el disco, disco I/O, etc). Si hacemos esto cada vez, nos costara mucho dinero.<br>\n",
    "Si el sistema necesita aprender automaticamente y tiene recursos limitados (como un celular o un rover en Marte), entocnces tomarse el largo tiempo de estar entrenando cada dia es un fallo critico.<br>\n",
    "<br>\n",
    "##### Online Learning\n",
    "En este tipo de aprendizaje , entrenamos el sistema dandole instancias de datos de manera secuencial, ya sea de manera individual o con pequeños grupos llamados mini-batches. Cada paso de aprendizaje es rapido y barato, asi que el sistema puede aprender sobre nuevos datos en el camino.<br>\n",
    "Online Learning es buenisimo para sistemas que reciben datos de manera constante y necesitan adaptarse al cambio de manera rapida y autonoma. Es tambien una buena opcion en caso de que se tenga una limitante en los recursos de hardware; una vez que el sistema de aprendizaje online ha aprendido sobre nuevas instancias de datos, no los necesita mas, asi que se los puede descartar. Esto nos ayuda mucho en el aspecto del espacio.<br>\n",
    "Los algoritmos de aprendizaje online tambien pueden ser usados para entrenar sistemas en datasets grandes que no pueden ajustarse en la memoria principal de una maquina (esti es llamado out-of-core learning). El algoritmo carga parte de los datos, los ejecuta en un proceso de entrenamiento en los dataos y repite el proceso hasta que que ejecuto todos los datos. <br>\n",
    "Puede ejecutar todo esto de manera offline tambien, por lo cual podemos pensar en este tipo de aprendizaje como \"incremental learning\".\n",
    "Un parametro importante en los sistemas de aprendizaje online es el que tan rapido estos se adaptan a los cambios en los datos: estos es llamado el learning rate. Si le damos un learning rate elevado, entonces el sisema se adaptara rapido a los nuevos datos, pero tendera a olvidar rapido los viejos datos. Por otro lado si le damos un learning rate bajo, el sistema tendra mas inercia, por lo cual aprendera mas lento, pero tambien sera menos sensible al ruido en los nuevos datos o al representar secuencias de puntos de datos no representativos(?).\n",
    "<br>\n",
    "Un gran reto en este tipo de aprendizaje es el de alimentar el sistema (sin querer) con datos erroneos, al hacer esto el sistema eventualmente caera en cuanto a su desempeño. <br>\n",
    "\n",
    "### Instance - Based Versus Model-Based Learning\n",
    "Esta es otra manera de ver como los sistemas ML generalizan. Muchas tareas en ML son sobre hacer prediciones. Esto significa que dado un numero de training examples, el sistema necesita estar disponible para generalizar ejemplos que nunca ha visto antes. Teniendo un buen desempeño de medida en el training data es bueno pero insuficiente; el verdadero objetivo es desempeñarse bien en nuevas instancias.<br>\n",
    "Hay dos aproximaciones principales para la generalizacion: <b>instance-based learning and model-based learning.</b>\n",
    "##### Instance-based Learning\n",
    "Posiblemente la manera mas trivial de aprendizaje es aprender con el corazon. Si quieres crear un filtro de spam de esta mane, marcarias todos los emails que sean identicos a los emails que han sido marcados como spam, no es la peor ni la mejor solucion. \n",
    "En este tipo de aprendizaje, se necesita la <b>medida de similitud</b> entre dos emails. Un medidad de similitud muy basica seria el de contar el numero de palabras que tienen en comun. El sistema podria marcar a los emails como spam si estos tienen tantas palabras como los emails que son spam.<br>\n",
    "Este tipo de sistema es llamdo instance-based learning, donde el sistema aprende los ejemplos con el corazon, despues generaliza los nuevos casos usando una medida de similitud.\n",
    "##### Model-based learning\n",
    "Otra manera de generalizar un conjunto de ejemplos es construyendo un modelo un modelo de estos ejemplos y despues usandolos para hacer predicciones. Esto es llamado model-based learning.\n",
    "Por ejemmplo supongamos que queremos saber si el dinero hace feliz a las personas, obtenemos una tabla donde figura el GDP (PIB) con la satisfaccion de vida de determinados paises. Haciendo esto podemos percatarnos de que la calidad de vida se mueve en conjunto con el PIB. Asi que decidimos manejar el modelo de calidad de vida como si se tratase de una funcion lineal del PIB. Este paso es llamado model selection: seleccionamos el modelo lineal de la calidad de vida con solo un atributo, el PIB.<br>\n",
    "<br>\n",
    "<br>\n",
    "<center>la ecuacion: calidad_de_vida = teta0 + teta1*PIB</center>\n",
    "<br>\n",
    "<br>\n",
    "Este modelo tiene dos parametros de modelo, teta0 y teta1. Retocando estos parametros, se puede hacer que el modelo represente cualquier funcion lineal.<br>\n",
    "Antes de poder usar nuestro modelo, debemos definir los valores para los parametros teta0 y teta1, pero como podemos saber que valores son los mejores? para responder esta pregunta necesitamos especificar la medida de desempeño. Bien podemos definir una funcion de utilidad que mida que tan bueno es nuestro modelo, o bien podemos definir una funcion de costo, que mida que tan malo es. Para problemas de regresion lineal, las personas usualmente usan  la funcion de coste que mide la distancia entre el modelo las predicciones del modelo lineal y los ejemplos de entrenamiento; el objetivo es el de minimizar la distancia.<br>\n",
    "Aqui es donde el algoritmo de Regresion Lineal entra en juego: lo alimentas con ejemplos de entrenamiento y este encuentra parametros que hacen que el modelo lineal se ajuste lo mejor a tus datos. Este proceso es llamado training (entrenando) el modelo. En nuestro caso el algoritmo encuentra que los valores optimos son: teta0 = 4.85, teta1 =  4.91 * 10**-5.\n",
    "<br>\n",
    "<br>\n",
    "Si las cosas no van bien con nuestro modelo de prediccion, debemos usar mas atributos, obtener mas y mejores datos de entrenamiento y tal vez seleccionar un modelo mas poderoso.<br>\n",
    "En resumen:\n",
    "<ul>\n",
    "    <li>Estudiamos los datos</li>\n",
    "    <li>Seleccionamos el modelo</li>\n",
    "    <li>Entrenamos el modelo con el training data (datos de entrenamiento)</li>\n",
    "    <li>Finalmente, aplicamos el modelo para hacer predicciones en nuevos casos (a esto se le llama inferencia), esperando que el modelo generalize bien.</li>\n",
    "</ul>\n",
    "<br>\n",
    "Asi es como un proyecto tipico de ML se ve.<br>\n",
    "\n",
    "## Principales retos en ML<br>\n",
    "\n",
    "Ya que la principal tarea es seleccionar un algoritmo de aprendizaje y entrenarlo con datos, dos cosas pueden salir mal,tener un \"mal algoritmo\" o tener \"malos datos\", empecemos con el ultimo:<br>\n",
    "\n",
    "### Malos datos\n",
    "<ol>\n",
    "    <li><b>Cantidad insuficiente de datos de entrenamiento:</b> Un algoritmo de ML necesita muchos datos para trabajar bien, incluso para problemas simples se necesitan miles de ejemplos.<br>\n",
    "        En un famoso papaer publicado en 2001, investigadores de Microsoft mostraton que en diferentes algoritmos de ML, se desempeñaron de casi una manera identica con un problema complejo de Natural Language Disambiguation una vez que estos recibian los datos suficientes. Los autores pusieron que \"estos resultados nos dicen que quiza deberiamos reconsiderar la compensacion entre invertir tiempo y dinero en el desarrollo versus invertir en el desarrollo del corpus(?)\".\n",
    "    La idea que los datos importan mas que los algoritmos para problemas complejos, aun asi es muy comun encontrarse con datasets pequeños a medianos, y no siempre es facil o barato conseguir mas datos, asi que aun no es buena idea abandonar los algoritmos(;)). \n",
    "    </li>\n",
    "    <li><b>\n",
    "Datos de entrenamiento no representativos (Nonrepresentative Training Data):</b> Es crucial que los datos de entrenamiento sean representativos de los nuevos casos que queremos generalizar. Esto es verdadero si queremos usar un aprendizaje basado en instancia o aprendizaje basado en modelos. <br>\n",
    "   Por ejemplo, el conjunto de paises que usamos antes para el entrenamiento del modelo lineal no era perfectamente representativo, algunos paises faltaban.<br>\n",
    "       Si entrenamos un modelo lineal en estos datos, obtendremos una linea solida, mientras la vieja linea es representada con lineas punteadas (en el grafico al que se refiere, el viejo modelo lineal que se obtuvo en el subtitulo de Model based learning esta ahora punteado haciendo referencia a que hay otro modelo lineal que se ajusta a todos los paises, pues ahora se tiene datos de mas paises). Como se puede ver, no solo añadiendo unos cuantos paises altera todo el modelo,  si no que nos hace dar cuenta que un modelo lienal simple probablemente nunca va a funcionar bien. Ademas con este nuevo modelo se puede ver que los paises mas ricos no son mas felices que los que no son tan ricos, es mas pareciera que los paises pobres son mas ricos. <br>\n",
    "       Usando un conjunto no representativo, entrenamos un modelo que no es bueno para hacer predicciones, especialmente para paises muy ricos o muy pobres.<br>\n",
    "        Es crucial elegrir un training set que sea representativo para los casos que queremos generalizar. Esto es mas dificil de lo que suena: si la muestra es muy pequeño, tendremos un ruido de muestreo, pero si la muestra es es muy grande puede no ser representativo si los metodos de muestreo son defectuosos. Esto es llamado bias de muestreo. <br>\n",
    "        <br>\n",
    "        <br>\n",
    "        <b>Un famoso ejemplo de sesgo de muestreo.</b><br>\n",
    "        Hace años en la eleccion presidencial de 1936, donde Landon se enfrentaba a Rossevelt, una revista condujo una gran encuesta, enviando un correo a unas 10 millones de personas. Recibio alrededor de 2.4 millones de respuestas, y predijeron con una gran confianza que Landon tendria el 57% de votos. Pero lo que paso fue que Roosevelt gano con el 62% de los votos. La falla estuvo en el metodo de la muestra de la revista:\n",
    "        <ul>\n",
    "            <li>Primero, obtener las direcciones para enviar la encuesta, la revista uso directorios de telefonos, directoios, listas de suscriptores de revistas, listas de miembros de club y entornos afines a estos. Todas estas listas estaban a favor de las personas ricas, quienes eran partidarias a votar por los republicanos (Landon).</li>\n",
    "            <li>Segundo, menos del 25% de las personas que recibieron las encuestas respondieron. Esto nos conduce a un sesgo de muestreo, mediante el descarte de personas que no les importa mucho la politica, personas que no les gusta la revista que esta haciendo la encuesta y otros grupos importantes. Esto es un tipo especial de bias llamado sesgo de no respuesta(?).</li>\n",
    "        </ul>\n",
    "    \n",
    "   </li>\n",
    "    <li><b>Calidad pobre de los datos</b>\n",
    "    <br>\n",
    "    Si los datos de entrenamiento estan llenos de erroes, ruido y muchas cosas asi, entonces sera dificil hacer que el sistema detecte patrones, asi que no es probable que trabaje bien. Es muy amenudo bien valorizado el esfuerzo en invertir tiempo para limpiar los datos. La verdad es que muchos cientificos de datos invierten un tiempo significativo en hacer esto. Por ejemplo:<br>\n",
    "    <ol>\n",
    "        <li>En algunos casos son claramente atipicos, asi que seria mejor simplemente descartarlos o intentar arreglarlos manualmente </li>\n",
    "        <li>En algunos casos faltan algunas caracteristicas (por ejemplo: 5% de los cliente sno pusieron sus edades), asi que debemos decidir en si lo ignoramos estos atributos, o lo rellenamos (por ejemplo, co la media de las edades), o entrenamos un modelo usando el atributo y otro modelo sin el atributo.</li>\n",
    "    </ol>\n",
    "</li>\n",
    "\n",
    "\n",
    "   <li><b>Features (Caracteristicas) Irrelevantes</b>\n",
    "   <br>\n",
    "    Como dice el dicho, basura entra y basura sale. EL sistema solo sera capaz de aprender si los datos de entrenamiento contienen las suficientes caracteristicas relevantes. Un factor crucial para el exito de un proyecto de Machine Learning es empezar con un buen conjunto de caracteristicas para entrenar. Este proceso, llamado <b>Feature Engineering</b> requiere:\n",
    "    <ul>\n",
    "    <li>\n",
    "            <b>Feature Selection (Seleccion de caracteristicas):</b> Donde seleccionamos las caracteristicas mas utiles para entrenar entre las caracteristicas existentes.\n",
    "    </li>\n",
    "        \n",
    "   <li><b>Feature Extraction (Extraccion de caracteristicas):</b>Combinando las caracteristicas existentes para producir una mucho mas util (como antes se vio, la reduccion de dimension ayuda bastante)</li>\n",
    "   \n",
    "   <li><b>Crear nuevas caracteristicas mediante la recopilacion de nuevos datos.</b></li>\n",
    "   </ul>\n",
    "\n",
    "</li>\n",
    "   \n",
    "</ol>\n",
    "\n",
    "\n",
    "### Malos algoritmos\n",
    "<ol>\n",
    "    <li>\n",
    "        <b>Overfitting en los datos de entrenamiento:</b> La generalizacion es algo que los humanos hacemos todo el tiempo, y desafortunadamente las maquinas pueden caer en el error de hacerlo, si es que no tenemos cuidado. En Machine Learning esto es llamado \"Overfitting\", lo cual significa que el modelo hace un buen trabajo en los datos de entrenamiento, pero no generaliza bien.<br>\n",
    "        En modelos complejos como las redes neuronales, se puede detectar patrones sutiles en los datos, pero si el training set es \"ruidoso\", o muy pequeño. Entonces el modelo detectara patrones en el ruido. Obviamente estos patrones no van a generalizar nuevos ejemplos. Por ejemplo si queremos poner mas datos en nuestro modelo de satisfaccion de vida, incluyendo algunos que son nada relevantes, como el nombre del pais. En ese caso, un modelo complejo puede detectar patrones como el hecho de que los paises del conjunto de entrenamiento que tienen una \"w\" en sus nombres, tienen una satisfaccion de vida arriba del 7/10. Pero que tan confiable es esto frente a paises que tienen la letra \"w\" en sus nombres? tales como Rwanda y Zimbawe? Obviamente este patron ocurrio en el conjunto de entrenamiento de pura casualidad, pero el modelo no tiene la manera de decir si es patron es real o un simple resultado del ruido en los datos.<br>\n",
    "   El overfitting pasa cuando el modelo es demasiado complejo en relacion a la cantidad de ruido y los datos de entrenamiento. Las posibles soluciones son:\n",
    "        <ul>\n",
    "            <li>Simplificar el modelo mediante la seleccion  de uno que tenga pocos parametros. (usar una regresion lineal en vez de una polinomial de alto grado), mediante la reduccion de numeros de atributos en el conjunto de datos, o restringiendo el modelo.\n",
    "            <li>Recogiendo muchos datos</li>\n",
    "            <li>Reduciendo el ruido en los datos de entrenamiento (arreglando los errores de los datos y eliminando los valores atípicos)</li>\n",
    "        </ul>\n",
    "El proceso de comprimir un modelo para hacerlo mas simple y de esta manera reducir el riesgo de sobreajuste, recibe el nombre de Regularización. Por ejemplo antes definimos un modelo con dos parametros: tetacero y tetauno. Esto le da al algoritmo de aprendizaje dos grados de libertad para que se adapte al modelo para entrenar los datos: donde se puede ajustar tanto en la altura (tetacero) y la pendiente (tetauno) de la linea. Si forzamos a que tetauno sea igual a cero, el algoritmo tendria solo un grado de libertad y le seria mucho mas dificil ajustarse a los datos de manera apropiada: todo lo que podria hacer es moverse hacia arriba o abajo para estar lo mas cerca posible de todas las instancias de entrenamiento. Si permitimos que conserve el valor de tetauno, pero forzamos sea un valor pequeño, entonces el algoritmo de aprendizaje tendra algo asi como de uno a dos grados de libertad. Producira un modelo simple comparado a un modelo con dos grados de libertad, pero tambien sera mas complejo comparado a un modelo con un grado de libertad. Lo que debemos hacer es encontrar el balance perfecto entre el ajuste de datos perfecto y mantener el modelo lo mas simple posible, asegurando que generalize bien.\n",
    "    \n",
    "  La cantidad de regularizacion a aplicar durante el aprendizaje puede ser controlado por un hiperparametro. Un hiperparametro es un parametro de un algoritmo de aprendizaje (no del modelo). Si el hiperparametro de regularizacion es un valor muy largo, tendremos un modelo plano (con una pendiente cerca  a cero) el algoritmo de aprendizaje no se sobreajustaria a los datos de entrenamiento, pero tampoco encontraria una buena solucion. Ajustar hiperparametros es importante para construir un sistema de machine learning.\n",
    "    \n",
    "   </li> \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
